#!/bin/bash
set -e
umask 002
#
# Script run on the dockerd host to start the A2KD Container.

function printLicense {
  cat <<-"EOF"
Copyright © 2012-2017 Raytheon BBN Technologies, Inc.
Cambridge, MA USA
All rights reserved.

This program and associated material contains information whose export or 
disclosure to Non-U.S. Persons, wherever located, is subject to the Export 
Administration Regulations (EAR) (15 C.F.R. §730-774). Specifically,
Raytheon BBN Technologies conducted an internal review and determined that this 
information is export controlled as EAR99. Exports contrary to U.S. law are
prohibited.

As part of the DEFT effort, BBN SERIF(TM) is being provided with Government
Purpose Rights. Please see DFARS 252.227-7014 for details.

Various third-party libraries are incorporated into this application. Please
see THIRD-PARTY.txt for a list of the libraries and their licenses.

-------------------------------------------------------------------------------

EOF
}

function version {
echo
printf "Version:           %s\n" ${git.build.version}
printf "branch:            %s\n" ${git.branch}
printf "tags:              %s\n" ${git.tags}
printf "commit time:       %s\n" ${git.commit.time}
printf "build time:        %s\n" ${git.build.time}
printf "closest tag count: %s\n" ${git.closest.tag.commit.count}
printf "closest tag:       %s\n" ${git.closest.tag.name}
printf "git commit:        %s\n" ${git.commit.id.abbrev}
printf "dirty:             %s\n" ${git.dirty}
echo
exit 0
}

function help {
cat <<EOF
a2kd

$1 <output directory path> <num_partitions> <num_dedup_partitions> <a2kd_config file> <spark settings file> <shared_top>

The $1 program executes an A2KD analysis over the provided corpus using the
parameters specified on the command line and in the configuration file.

This program requires access to the HADOOP configuration
directory in order to function properly.

If the standard environment variable HADOOP_CONF_DIR is not 
defined, the program will look for the hadoop configuration directory at 
$HOME/hadoop/conf, then /etc/hadoop/conf. If these directories cannot be found
and the environment variable is not defined, the program will fail.

 If you have this directory in a non-standard location, the
environment variables can easily be defined on the command line:

HADOOP_CONF_DIR=/share/hadoop/conf $1 output 5 5 a2kd_config.xml spark.conf /nfs/gold-04/u40

You MAY specify a Spark configuration directory to pick up the default Spark setting for
your cluster. Use the SPARK_CONF_DIR environment variable for this purpose.

An image with a different version than latest may be used in place of deft/a2kd:latest
by specifying the version with the A2KD_VERSION environment variable. For instance,

A2KD_VERSION=170803-201933-0233 runa2kd ...

will attempt to use the deft/a2kd:170803-201933-0233 image from your repository.

An image with a completely different name or in a repository other then your local
repository can be used as well. Simply set the DEFTIMAGE environment variable to
the name of your image:

DEFTIMAGE=deft-images-host:5000/deft/a2kd:latest runa2kd ...

If both DEFTIMAGE and A2KD_VERSION are set, A2KD_VERSION is ignored.

The parameters that must be defined in the configuration file are documented therein.

The spark_config file contents are documented in 
https://spark.apache.org/docs/2.1.0/configuration.html#loading-default-configurations.
See the Spark Documentation for information on the parameters that can be specified.

The path described by shared_top is the top-level directory of a directory tree 
whose path is common to all nodes in the cluster.  
EOF
[ $# -eq 2 ] && exit $2
}

function errexit {
   ( >&2 echo "$1")
   logger -p user.error "$1"
   rm -rf /tmp/input$$
   exit 1
}

# check for help
if [ $# -eq 0 -o "$1" = "-h" -o "$1" = "--help" ]; then
  help $(basename $0) 0
fi
if [ "$1" = "-v" -o "$1" = "--version" ]; then
  version
fi

if [ $# -ne 6 ]; then
  ( >&2 echo "\nERROR: incorrect number of parameters: $#")
  help $(basename $0) 1
fi

# check for xmllint
hash xmllint 2>/dev/null || { echo >&2 "I require xmllint but it's not installed or in your PATH.  Aborting."; exit 1; }
if [ x"$DOCKER_CMD" = x ]; then
  hash docker 2>/dev/null || { echo >&2 "I require docker but it's not installed or in your PATH.  Aborting."; exit 1; }
fi

job_timestamp=$(date +%y%m%d%H%M%S)
echo "The UTC job_timestamp/id of this job is $job_timestamp"

# Set up identity variables
UI="$(id -u)"
UN="$(echo -n "$(id -un)" | tr "[:space:]" "_")"
GI="$(id -g)"
GN="$(echo -n "$(id -gn)" | tr "[:space:]" "_")"

# $1 output directory - create if not there
if [ ! -e "${1}" ] ; then
  mkdir -p "${1}"
  chmod 775 "${1}"
fi
[ -d "${1}" -a -w "${1}" ] || errexit "ERROR: path \"$1\" is not a directory or is not writable"
output_dir=$(readlink -f "${1}")
 
[[ "$2" =~ ^[0-9]+$ ]] || errexit "ERROR: value \"$2\" specified for number of partitions is not an integer"
num_partitions=$2 

[[ "$3" =~ ^[0-9]+$ ]] || errexit "ERROR: value \"$3\" specified for number of deduplication partitions is not an integer"
num_dd_partitions=$3

# $4 a2kd config file
if [ ! -f "${4}" -o ! -r "${4}" ] ; then
  errexit "ERROR: A2KD Configuration File \"${4}\" does not exist, is not a file, or is not readable"
fi
a2kd_config="$4"

# $5 spark config file
if [ ! -f "${5}" -o ! -r "${5}" ] ; then
  errexit "ERROR: Spark Configuration File \"${5}\" does not exist, is not a file, or is not readable"
fi
spark_config="$5"

# $6 shared directory
[ -e "${6}" ] || errexit "ERROR: Shared Directory \"${6}\" does not exist"
[ -d "${6}" ] || errexit "ERROR: Shared Directory path \"${6}\" is not a directory"
[ -r "${6}" ] || errexit "ERROR: Shared Directory \"${6}\" is not readable"
[ -x "${6}" ] || errexit "ERROR: Shared Directory \"${6}\" is not searchable"
shared_top="$6"
# create per-user shared directory if needed
shared_directory="${6}/$(id -un)"
if [ ! -d "$shared_directory" ]; then
  mkdir -p "$shared_directory" || errexit "ERROR: Could not create User Shared Directory $shared_directory"
fi
# Now create the job directory under that
job_directory="${shared_directory}/job_${job_timestamp}"
[ -e "$job_directory" ] && errexit "ERROR: Job Directory $job_directory already exists!"
mkdir -p "$job_directory" || errexit "ERROR: Could not create Job Directory $job_directory"

# populate the job directory with our input info
echo "$num_partitions" >"${job_directory}/partitions"
echo "$num_dd_partitions" >"${job_directory}/ddPartitions"
echo "$job_directory" >"${job_directory}/job_directory"
echo "$job_timestamp" >"${job_directory}/job_timestamp"
cp "$a2kd_config" "${job_directory}/config.xml"
cp "$spark_config" "${job_directory}/spark.conf"
spark_config="${job_directory}/spark.conf"

# Get the language(s) and their input directories so we can mount them in the Docker container
dirs=$(echo 'cat /config/algorithm_set/input_directory/text()' | xmllint --shell "$a2kd_config" | grep -vE "^(/ > | ---)")
input_dirs=( $dirs )
langs=$(echo 'cat /config/algorithm_set/@language' | xmllint --shell "$a2kd_config" | grep -vE "^(/ > | ---)" | sed -e 's/^[^\"]*\"//' -e 's/\"//g')
languages=( $langs )
echo $langs >"${job_directory}/languages"

[ ${languages:-none} = none ] && errexit "ERROR: no input languages specified in the configuration file"
[ ${input_dirs:-none} = none ] && errexit "ERROR: no input directory specified in the configuration file"
rm -f "${job_directory}/mounts"
cnt=0
for dir in $dirs; do
  [ -e "${dir}" ] ||  errexit "ERROR: input directory \"${dir}\" for language \"${languages[$cnt]}\" does not exist"
  [ -d "${dir}" ] ||  errexit "ERROR: input directory path \"${dir}\" for language \"${languages[$cnt]}\" exists but is not a directory"
  [ -r "${dir}" ] ||  errexit "ERROR: input directory path \"${dir}\" for language \"${languages[$cnt]}\" exists but is not readable"
  [ "$(ls -A ${dir})" ] || errexit "ERROR: input directory path \"${dir}\" for language \"${languages[$cnt]}\" is empty - no input files found"
  # This is building VOLUME arguments for the docker run command below
  printf -- '-v %s:/%s ' "$(readlink -f "$dir")" "${languages[$cnt]}" >>"${job_directory}/mounts"
  cnt=$(( $cnt + 1 ))
done

chmod ug=rw,o=r ${job_directory}/*

[ -f log4j.properties ] && cp log4j.properties ${job_directory}

# Get corpus_id to use as docker container name
corpus_id=$(echo 'cat /config/kb_config/@corpus_id' | xmllint --shell "$a2kd_config" | grep -vE "^(/ > | ---)" | sed -n 's/[^\"]*\"\([^\"]*\)\"[^\"]*/\1/gp')

# Check for or set HADOOP_CONF_DIR
if [  x"$HADOOP_CONF_DIR" = x ] ; then
  for HCD in /etc/hadoop/conf ~/hadoop/conf /var/lib/hadoop/conf; do
    if [ -d $HCD -a -r $HCD ]; then
      HADOOP_CONF_DIR="$HCD"
      break
    fi
  done
  if [ x"$HADOOP_CONF_DIR" = x -a x"$HADOOP_HOME" != x ] ; then
    if [ -d "$HADOOP_HOME/conf" -a -r "$HADOOP_HOME/conf" -a -x "$HADOOP_HOME/conf"] ; then
      HADOOP_CONF_DIR="$HADOOP_HOME/conf"
    fi
  fi
fi

[ "$HADOOP_CONF_DIR" ] || errexit "ERROR: HADOOP_CONF_DIR is not defined and cannot be guessed"
[ -e "$HADOOP_CONF_DIR" ] || errexit "ERROR: HADOOP_CONF_DIR $HADOOP_CONF_DIR does not exist"
[ -d "$HADOOP_CONF_DIR" ] || errexit "ERROR: HADOOP_CONF_DIR $HADOOP_CONF_DIR is not a directory"
[ -r "$HADOOP_CONF_DIR" ] || errexit "ERROR: HADOOP_CONF_DIR $HADOOP_CONF_DIR found but is not readable"
[ -x "$HADOOP_CONF_DIR" ] || errexit "ERROR: HADOOP_CONF_DIR $HADOOP_CONF_DIR found but is not searchable"

if [ x"$SPARK_CONF_DIR" != x ] ; then
  [ -e "$SPARK_CONF_DIR" ] || errexit "ERROR: SPARK_CONF_DIR $SPARK_CONF_DIR does not exist"
  [ -d "$SPARK_CONF_DIR" ] || errexit "ERROR: SPARK_CONF_DIR $SPARK_CONF_DIR is not a directory"
  [ -r "$SPARK_CONF_DIR" ] || errexit "ERROR: SPARK_CONF_DIR $SPARK_CONF_DIR found but is not readable"
  [ -x "$SPARK_CONF_DIR" ] || errexit "ERROR: SPARK_CONF_DIR $SPARK_CONF_DIR found but is not searchable"
fi
# Fixup the class path iff necessary
# Extract the classpath from the spark configuration file
SCP=$(awk '/spark.driver.extraClassPath/ {print $2}' "${spark_config}" )
if [ "x$SCP" = x ]; then
  SCP=$(awk '/spark.executor.extraClassPath/ {print $2}' "${spark_config}" )
fi
if [ "x$SCP" = x ]; then
  SCP=$(awk '/extraClassPath/ {print $2}' "${spark_config}" )
fi
if [ "x$SCP" = x ]; then
  echo "ERROR: ${0}: extraClassPath definition not found in spark configuration"
  exit 1
fi

set -o noglob
cnt=0
dirs=$(echo ${SCP:-x} | tr ':' ' ')
# convert to an array
for dir in $dirs; do
  dir=${dir//\/\*/}
  [ ${dir: -4} == ".jar" ] && continue
  [ -e "$dir" ] || errexit "ERROR: path \"$dir\" in spark classpath does not exist"
  [ -d "$dir" ] || errexit "ERROR: path \"$dir\" in spark classpath is not a directory"
  [ -r "$dir" ] || errexit "ERROR: directory \"$dir\" in spark classpath is not readable"
  [ -x "$dir" ] || errexit "ERROR: directory \"$dir\" in spark classpath is not searchable"
  # The first must be our CLASSPATH_TOP with a following asterisk
  if [ -d "${dir}/com/bbn/serif" ]; then
    CLASSPATH_TOP="$(dirname $dir)"
  fi
done
first="${job_directory}/classes"
if [ ! -d "$first" ] ; then
  mkdir -p "$first" || errexit "ERROR: could not create $first"
fi
set +o noglob
if [ "${CLASSPATH_TOP:-x}" != x ]; then
  dir="$CLASSPATH_TOP/classes"
  for fileToProcess in $dir/*.template.*; do
    # get file name sans the .template.
    bn=$(basename $fileToProcess)
    targetFile=$(echo $bn | sed 's!\.template!!g')
    targetFile="${first}/$targetFile"
    sed -e "s!\$CURDIR!${first}!g" -e "s!\$CLASSPATH_TOP!$CLASSPATH_TOP!g" $fileToProcess >$targetFile
  done
  if grep -qF spark.driver.extraClassPath "${spark_config}" ; then
    sed -i -e 's!^\s*\(spark.driver.extraClassPath\s*\)\(\w*\)!\1'"$first"':\2!' "${spark_config}"
  else
    echo "spark.driver.extraClassPath   ${first}:${SCP}" >> "${spark_config}"
  fi
  if grep -qF spark.executor.extraClassPath "${spark_config}" ; then
    sed -i -e 's!^\s*\(spark.executor.extraClassPath\s*\)\(\w*\)!\1'"$first"':\2!' "${spark_config}"
  else
    echo "spark.executor.extraClassPath ${first}:${SCP}" >> "${spark_config}"
  fi
fi
# set files up if log4j.properties is present
if ! grep -F log4j.properties  "${spark_config}" 2>&1 1>/dev/null ; then
  if [ -f "${job_directory}/log4j.properties" ]; then
    cat >> "${spark_config}" <<EOF
spark.executor.extraJavaOptions -Dlog4j.configuration="${job_directory}/log4j.properties"
spark.driver.extraJavaOptions   -Dlog4j.configuration="${job_directory}/log4j.properties"
EOF
  fi
fi

myPort=4040
while $(netstat -tln | grep -qF ":$myPort ") ; do
  myPort=$(( myPort + 1 ))
done
# print the license
printLicense
echo
echo starting Docker container with port $myPort exposed

# Next Phase - start the container and continue processing
# job_directory contains:
#   $job_directory/partitions   - a file containing the number of partitions to use.
#   $job_directory/ddPartitions - a file containing the number of document deduplication partitions to use.
#   $job_directory/config.xml   - a file containing the A2KD Configuration file.
#   $job_directory/spark.conf   - a file containing the Spark properties file.
#   $job_directory/languages    - a file containing a space-separated list of language codes
# The docker container will have the following volumes:
#   /input - mount to $job_directory containing the files listed above
#   /output - mount to output directory
#   /{EN|ZH|ES} - mounts to the directories containing input files - the name being a language code
#   /hadoop - a mount to the HADOOP configuration directory
#   /sharedTop - a mount to the shared_top directory
#
if [ x"$DEFTIMAGE" == "x" ]; then
  if [ x"$A2KD_VERSION" == "x" ]; then
    DEFTA2KD="deft/a2kd:latest"
  else
    DEFTA2KD="deft/a2kd:${A2KD_VERSION}"
    if ! docker inspect "$DEFTA2KD" >/dev/null 2>&1 ; then
      DEFTA2KD="${UN}/a2kd:${A2KD_VERSION}"
      if ! docker inspect "$DEFTA2KD" >/dev/null 2>&1 ; then
        echo "A2KD_VERSION is set to \"${A2KD_VERSION}\", but no image named \"deft/a2kd:${A2KD_VERSION}\" or \"${DEFTA2KD}\" can be found"
        exit 1
      fi
    fi
  fi
else
  if ! docker inspect "$DEFTIMAGE" >/dev/null 2>&1 ; then
    echo "DEFTIMAGE is set to \"${DEFTIMAGE}\", but no image with that name can be found!"
    exit 1
  fi
  DEFTA2KD=$DEFTIMAGE
fi

echo "Running docker image ${DEFTA2KD}" 

${DOCKER_CMD:-docker} run -it --rm --name "$UN-$corpus_id" \
  -e LOCAL_USER_ID=$UI \
  -e LOCAL_USER_NAME="$UN" \
  -e LOCAL_GROUP_ID=$GI \
  -e LOCAL_GROUP_NAME="$GN" \
  -e shared_top="$shared_top" \
  -e job_timestamp="$job_timestamp" \
  -e job_directory="$job_directory" \
  -v $job_directory:/input \
  -v $shared_top:/sharedTop \
  -v $output_dir:/output \
  -v $HADOOP_CONF_DIR:/hadoop \
  $(cat ${job_directory}/mounts) \
  -p $myPort:4040 \
  ${DEFTA2KD} a2kd.sh

